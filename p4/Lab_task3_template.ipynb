{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518d9af0",
   "metadata": {
    "id": "518d9af0"
   },
   "source": [
    "# **Task 3: Post-Training Quantization with SmoothQuant**\n",
    "\n",
    "## **Overview**\n",
    "Implements and evaluates **SmoothQuant**, an advanced **post-training quantization (PTQ)** method for large language models.  \n",
    "The notebook diagnoses why naive quantization struggles, applies activation/weight smoothing, and measures the impact using **Perplexity (PPL)** on the **Wikitext** dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Environment and Data Preparation**\n",
    "\n",
    "- **Baseline model:**  \n",
    "  Load a full-precision **BF16** model as the gold-standard reference.\n",
    "\n",
    "- **Dataset split:**  \n",
    "  Load **Wikitext** and create two subsets:\n",
    "  - **Calibration:** A small portion of the training set used to analyze activations and compute SmoothQuant scaling factors.  \n",
    "  - **Evaluation:** The test set held out for fair PPL evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Diagnosing the Quantization Challenge**\n",
    "\n",
    "- **Activation capture:**  \n",
    "  Use **forward hooks** to record input activations of selected linear layers on calibration batches.\n",
    "\n",
    "- **Distribution plots:**  \n",
    "  For each target layer, show side-by-side histograms of  \n",
    "  (a) **weights** and  \n",
    "  (b) **input activations**  \n",
    "  to illustrate why standard quantization is difficult.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Implementing the SmoothQuant Toolkit**\n",
    "\n",
    "- **Basic quantizers:**  \n",
    "  Implement **per-channel weight quantization (int8)** and **per-token activation quantization (int8)**.\n",
    "\n",
    "- **Quantized layer:**  \n",
    "  Define **`WnAnLinear`**, a drop-in `nn.Linear` replacement that stores quantized weights and dynamically quantizes activations in its forward pass.\n",
    "\n",
    "- **Smoothing core:**  \n",
    "  Implement **`smooth_ln_fcs`**, which scales activations down and weights up using factors derived from their distributions—shifting quantization difficulty from activations to weights.\n",
    "\n",
    "- **Model wrappers:**  \n",
    "  - **`smooth_model`** applies smoothing across the model.  \n",
    "  - **`quantize_model`** replaces eligible linear layers with the quantized variant.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Calibration and Evaluation Workflow**\n",
    "\n",
    "- **Activation scaling:**  \n",
    "  **`get_act_scales`** runs the calibration set with hooks to compute per-channel activation maxima for smoothing.\n",
    "\n",
    "- **Perplexity evaluator:**  \n",
    "  **`Evaluator`** tokenizes data, computes loss, and reports **PPL** (lower is better) on the **Wikitext** test set.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Main Experiments**\n",
    "\n",
    "- **Configurations:**  \n",
    "  For example define runs for:\n",
    "  - **BF16 baseline**\n",
    "  - **Naive W8A8**\n",
    "  - **W8A8 + SmoothQuant**\n",
    "\n",
    "- **Orchestration:**  \n",
    "  **`run_experiment`** loads the model, optionally smooths, quantizes, and evaluates PPL.\n",
    "\n",
    "- **Models:**  \n",
    "  Execute across multiple LLMs (e.g., **Llama-3-8B** and **Llama-2-7B**) and record results.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Results and Conclusions**\n",
    "\n",
    "- **Aggregation:**  \n",
    "  Collect PPLs into a **pandas DataFrame** and print a concise summary table to compare **SmoothQuant**, **naive W8A8**, and the **BF16 baseline**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66dd510d",
   "metadata": {
    "id": "66dd510d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tchopra32/Programming/eml/p4/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Environment setup and dependency installation complete.\n"
     ]
    }
   ],
   "source": [
    "### Cell 2: Environment Setup and Dependency Installation\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "from typing import Optional, Tuple, Callable\n",
    "import types\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from scipy.stats import linregress\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.generation.stopping_criteria import (\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    ")\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaAttention,\n",
    "    rotate_half,\n",
    "    repeat_kv,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "\n",
    "RESULTS_DIR = \"./results\"\n",
    "FIGURES_DIR = \"./figures\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility across Python, NumPy, and PyTorch.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"\\n   Environment setup and dependency installation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bLRYYED4G2U0",
   "metadata": {
    "id": "bLRYYED4G2U0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "   Hugging Face login successful!\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 3: Hugging Face Login\n",
    "from huggingface_hub import login, HfFolder\n",
    "from getpass import getpass\n",
    "\n",
    "# Check if a Hugging Face token is already set in the environment.\n",
    "if not os.getenv(\"HUGGING_FACE_HUB_TOKEN\"):\n",
    "    try:\n",
    "        # Prompt user for Hugging Face access token if not found.\n",
    "        hf_token = getpass(\"Please enter your Hugging Face access token: \")\n",
    "        login(token=hf_token, add_to_git_credential=True)\n",
    "        print(\"   Hugging Face login successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}. Model loading may fail later.\")\n",
    "else:\n",
    "    print(\"   Hugging Face token detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "od8wgKjiG2U0",
   "metadata": {
    "id": "od8wgKjiG2U0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading bf16 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Wikitext dataset...\n",
      "   Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "### Cell 4: Model, Tokenizer, and Dataset Loading\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "def load_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "\n",
    "# Task 3, Step 1: Load the baseline BF16 model for quantization experiments\n",
    "print(\"\\nLoading bf16 model...\")\n",
    "model_fp16, tokenizer = load_model_and_tokenizer(MODEL_ID)\n",
    "\n",
    "# Task 3, Step 1: Load the Wikitext dataset for calibration and evaluation\n",
    "print(\"\\nLoading Wikitext dataset...\")\n",
    "raw_datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Very small train subset for calibration and full test for eval (can shrink if OOM)\n",
    "calibration_dataset = raw_datasets[\"train\"].select(range(512))\n",
    "eval_dataset = raw_datasets[\"test\"]\n",
    "print(\"   Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qxJFZrl-G2U1",
   "metadata": {
    "id": "qxJFZrl-G2U1"
   },
   "outputs": [],
   "source": [
    "### Cell 5: Visualization of Weight and Activation Distributions\n",
    "\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "def visualize_distributions(model, tokenizer):\n",
    "\n",
    "    CALIBRATION_SAMPLES = 64\n",
    "    SEQ_LEN = 128\n",
    "    NUM_BINS = 80\n",
    "\n",
    "    LAYERS_TO_VISUALIZE = [\n",
    "        \"model.layers.0.self_attn.o_proj\",\n",
    "        \"model.layers.0.mlp.up_proj\",\n",
    "        \"model.layers.10.self_attn.o_proj\",\n",
    "        \"model.layers.10.mlp.up_proj\",\n",
    "        \"model.layers.20.self_attn.o_proj\",\n",
    "        \"model.layers.20.mlp.up_proj\",\n",
    "        \"model.layers.30.self_attn.o_proj\",\n",
    "        \"model.layers.30.mlp.up_proj\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    modules = dict(model.named_modules())\n",
    "    activations = defaultdict(list)\n",
    "    handles = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook(mod, inputs, output):\n",
    "            x = inputs[0].detach()\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(1)\n",
    "            activations[name].append(x.cpu())\n",
    "        return hook\n",
    "\n",
    "    for lname in LAYERS_TO_VISUALIZE:\n",
    "        if lname in modules and isinstance(modules[lname], nn.Linear):\n",
    "            h = modules[lname].register_forward_hook(make_hook(lname))\n",
    "            handles.append(h)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seen = 0\n",
    "        for example in calibration_dataset:\n",
    "            if seen >= CALIBRATION_SAMPLES:\n",
    "                break\n",
    "            text = example[\"text\"].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=SEQ_LEN,\n",
    "            )\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            _ = model(**enc)\n",
    "            seen += 1\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    for lname in LAYERS_TO_VISUALIZE:\n",
    "        if lname not in modules or lname not in activations:\n",
    "            continue\n",
    "\n",
    "        layer = modules[lname]\n",
    "\n",
    "        w = layer.weight.detach().float().cpu().numpy().reshape(-1)\n",
    "        al = [t.reshape(-1, t.shape[-1]) for t in activations[lname]]\n",
    "        a = torch.cat(al, dim=0).float().cpu().numpy().reshape(-1)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        axes[0].hist(w, bins=NUM_BINS, log=True)\n",
    "        axes[0].set_title(\"Weights\")\n",
    "        axes[0].set_xlabel(\"Value\")\n",
    "        axes[0].set_ylabel(\"Count (log)\")\n",
    "\n",
    "        axes[1].hist(a, bins=NUM_BINS, log=True)\n",
    "        axes[1].set_title(\"Activations\")\n",
    "        axes[1].set_xlabel(\"Value\")\n",
    "        axes[1].set_ylabel(\"Count (log)\")\n",
    "\n",
    "        fig.suptitle(f\"Weight vs Activation Distributions\\n{lname}\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"{FIGURES_DIR}/task3_step1__{lname.replace('.', '_')}.png\", dpi=120)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def visualize_distributions_3d(model, tokenizer):\n",
    "    \n",
    "    CALIBRATION_SAMPLES = 1\n",
    "    SEQ_LEN = 128\n",
    "    LAYERS_TO_VISUALIZE = [\n",
    "        \"model.layers.0.self_attn.o_proj\",\n",
    "        \"model.layers.0.mlp.up_proj\",\n",
    "    ]\n",
    "\n",
    "    modules = dict(model.named_modules())\n",
    "    act_snapshot = {}\n",
    "    handles = []\n",
    "\n",
    "    def make_hook(name):\n",
    "        def hook(mod, inputs, output):\n",
    "            if name in act_snapshot:\n",
    "                return\n",
    "            x = inputs[0].detach()\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(1)\n",
    "            act_snapshot[name] = x[0].cpu()  # (L, H) from batch 0\n",
    "        return hook\n",
    "\n",
    "    for lname in LAYERS_TO_VISUALIZE:\n",
    "        if lname in modules and isinstance(modules[lname], nn.Linear):\n",
    "            h = modules[lname].register_forward_hook(make_hook(lname))\n",
    "            handles.append(h)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seen = 0\n",
    "        for example in calibration_dataset:\n",
    "            if seen >= CALIBRATION_SAMPLES:\n",
    "                break\n",
    "            text = example[\"text\"].strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            enc = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=SEQ_LEN,\n",
    "            )\n",
    "            enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "            _ = model(**enc)\n",
    "            seen += 1\n",
    "\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "\n",
    "    for lname, act in act_snapshot.items():\n",
    "        L, H = act.shape\n",
    "        max_h = min(64, H)\n",
    "        act_sub = act[:, :max_h]\n",
    "\n",
    "        X = np.arange(L)\n",
    "        Y = np.arange(max_h)\n",
    "        X, Y = np.meshgrid(X, Y)\n",
    "        Z = act_sub.T.float().cpu().numpy()\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 6))\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        ax.plot_surface(X, Y, Z, linewidth=0, antialiased=True)\n",
    "        ax.set_title(f\"3D Activation Surface – {lname}\")\n",
    "        ax.set_xlabel(\"Token position\")\n",
    "        ax.set_ylabel(\"Hidden dim (subset)\")\n",
    "        ax.set_zlabel(\"Activation\")\n",
    "\n",
    "        fig.savefig(f\"{FIGURES_DIR}/task3_step1__{lname.replace('.', '_')}_3d.png\", dpi=120)\n",
    "        plt.close(fig)\n",
    "\n",
    "# Task 3, Step 2: Visualize weight and activation distributions to motivate SmoothQuant\n",
    "visualize_distributions(model_fp16, tokenizer)\n",
    "visualize_distributions_3d(model_fp16, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mBof2UwvG2U1",
   "metadata": {
    "id": "mBof2UwvG2U1"
   },
   "outputs": [],
   "source": [
    "### Cell 6: Core Implementation of SmoothQuant\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Part 1: Quantizers\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "    \"\"\"\n",
    "    Quantizes weights per output channel using absolute max scaling.\n",
    "    Assumes w is (out_features, in_features).\n",
    "    \"\"\"\n",
    "    qmax = 2 ** (n_bits - 1) - 1\n",
    "    max_vals = w.abs().amax(dim=1, keepdim=True)  # (out, 1)\n",
    "    max_vals = torch.clamp(max_vals, min=1e-8)\n",
    "    scales = max_vals / qmax\n",
    "    w_int = torch.round(w / scales).clamp(-qmax - 1, qmax).to(torch.int8)\n",
    "    return w_int, scales.squeeze(1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    \"\"\"\n",
    "    Quantizes activations per token using absolute max scaling.\n",
    "    Supports (B, L, H) or (B, H) tensors.\n",
    "    \"\"\"\n",
    "    qmax = 2 ** (n_bits - 1) - 1\n",
    "    if t.dim() == 2:\n",
    "        t = t.unsqueeze(1)  # (B, 1, H)\n",
    "        squeeze_back = True\n",
    "    else:\n",
    "        squeeze_back = False\n",
    "\n",
    "    max_vals = t.abs().amax(dim=-1, keepdim=True)  # (..., 1)\n",
    "    max_vals = torch.clamp(max_vals, min=1e-8)\n",
    "    scales = max_vals / qmax\n",
    "    t_int = torch.round(t / scales).clamp(-qmax - 1, qmax).to(torch.int8)\n",
    "\n",
    "    if squeeze_back:\n",
    "        t_int = t_int.squeeze(1)\n",
    "        scales = scales.squeeze(1)\n",
    "\n",
    "    return t_int, scales\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Part 2: Quantized Linear Layer\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "class WnAnLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantized Linear Layer with per-channel weight and per-token activation quantization.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, w_bits=8, a_bits=8):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.w_bits = w_bits\n",
    "        self.a_bits = a_bits\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight_int\",\n",
    "            torch.zeros(out_features, in_features, dtype=torch.int8),\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"weight_scale\",\n",
    "            torch.ones(out_features, dtype=torch.float32),\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features, dtype=torch.float32))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_int, x_scale = quantize_activation_per_token_absmax(x, self.a_bits)\n",
    "        x_deq = x_int.float() * x_scale\n",
    "        w_deq = self.weight_int.float() * self.weight_scale.unsqueeze(1)\n",
    "        out = F.linear(x_deq, w_deq, self.bias)\n",
    "        return out.to(x.dtype)\n",
    "\n",
    "    @classmethod\n",
    "    @torch.no_grad()\n",
    "    def from_float(cls, module, w_bits=8, a_bits=8):\n",
    "        assert isinstance(module, nn.Linear)\n",
    "        qmod = cls(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            bias=module.bias is not None,\n",
    "            w_bits=w_bits,\n",
    "            a_bits=a_bits,\n",
    "        )\n",
    "        w = module.weight.detach()\n",
    "        w_int, w_scale = quantize_weight_per_channel_absmax(w, n_bits=w_bits)\n",
    "        qmod.weight_int.copy_(w_int)\n",
    "        qmod.weight_scale.copy_(w_scale.to(qmod.weight_scale.dtype))\n",
    "        if module.bias is not None:\n",
    "            qmod.bias.data.copy_(module.bias.detach().to(qmod.bias.dtype))\n",
    "        return qmod\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Part 3: Smoothing Function (SmoothQuant)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_ln_fcs(ln, fcs, act_scales, alpha=0.5):\n",
    "    hidden_dim = ln.normalized_shape[0]\n",
    "    assert act_scales.numel() == hidden_dim\n",
    "    weight_max = torch.zeros(hidden_dim, device=act_scales.device)\n",
    "    for fc in fcs:\n",
    "        w = fc.weight.detach()  # (out, hidden_dim)\n",
    "        weight_max = torch.maximum(weight_max, w.abs().amax(dim=0))\n",
    "    weight_max = torch.clamp(weight_max, min=1e-8)\n",
    "    act_max = torch.clamp(act_scales.to(weight_max.device), min=1e-8)\n",
    "    s = (act_max ** alpha) / (weight_max ** (1 - alpha))\n",
    "    s = torch.clamp(s, min=1e-4, max=1e4)\n",
    "    if ln.weight is not None:\n",
    "        ln.weight.data /= s\n",
    "    if ln.bias is not None:\n",
    "        ln.bias.data /= s\n",
    "    for fc in fcs:\n",
    "        fc.weight.data *= s.unsqueeze(0)\n",
    "    return s\n",
    "\n",
    "\n",
    "def find_layers(module, layers=(nn.Linear,), name=\"\"):\n",
    "    out = {}\n",
    "    for child_name, child in module.named_children():\n",
    "        full = f\"{name}.{child_name}\" if name else child_name\n",
    "        if isinstance(child, layers):\n",
    "            out[full] = child\n",
    "        else:\n",
    "            out.update(find_layers(child, layers, full))\n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_model(model, act_scales, alpha=0.5):\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        ln1_name = f\"model.layers.{i}.input_layernorm\"\n",
    "        ln2_name = f\"model.layers.{i}.post_attention_layernorm\"\n",
    "\n",
    "        if ln1_name in act_scales:\n",
    "            ln = layer.input_layernorm\n",
    "            fcs = [\n",
    "                layer.self_attn.q_proj,\n",
    "                layer.self_attn.k_proj,\n",
    "                layer.self_attn.v_proj,\n",
    "                layer.self_attn.o_proj,\n",
    "            ]\n",
    "            smooth_ln_fcs(ln, fcs, act_scales[ln1_name], alpha=alpha)\n",
    "\n",
    "        if ln2_name in act_scales:\n",
    "            ln = layer.post_attention_layernorm\n",
    "            fcs = [\n",
    "                layer.mlp.gate_proj,\n",
    "                layer.mlp.up_proj,\n",
    "                layer.mlp.down_proj,\n",
    "            ]\n",
    "            smooth_ln_fcs(ln, fcs, act_scales[ln2_name], alpha=alpha)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def quantize_model(model, w_bits=8, a_bits=8):\n",
    "    \"\"\"\n",
    "    Replaces Llama linear layers with WnAnLinear.\n",
    "    \"\"\"\n",
    "    for module_name, module in list(model.named_modules()):\n",
    "        for child_name, child in list(module.named_children()):\n",
    "            if isinstance(child, nn.Linear):\n",
    "                setattr(\n",
    "                    module,\n",
    "                    child_name,\n",
    "                    WnAnLinear.from_float(child, w_bits=w_bits, a_bits=a_bits),\n",
    "                )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "VzL0rPZwG2U2",
   "metadata": {
    "id": "VzL0rPZwG2U2"
   },
   "outputs": [],
   "source": [
    "### Cell 7: Activation Scale Calibration & Perplexity Evaluation\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Part 1: Activation Scale Calibration\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "import math\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_act_scales(model, tokenizer, dataset, num_samples=256, seq_len=512):\n",
    "    ln_modules = {\n",
    "        name: m for name, m in model.named_modules() if isinstance(m, nn.LayerNorm)\n",
    "    }\n",
    "    act_scales = {\n",
    "        name: torch.zeros(m.normalized_shape[0], device=DEVICE)\n",
    "        for name, m in ln_modules.items()\n",
    "    }\n",
    "    handles = []\n",
    "    def make_hook(name):\n",
    "        def hook(mod, inputs, output):\n",
    "            x = output.detach()\n",
    "            if x.dim() == 2:\n",
    "                x = x.unsqueeze(1)\n",
    "            x = x.reshape(-1, x.shape[-1])  # (B*L, H)\n",
    "            max_abs = x.abs().amax(dim=0)\n",
    "            act_scales[name] = torch.maximum(act_scales[name], max_abs)\n",
    "        return hook\n",
    "    for name, ln in ln_modules.items():\n",
    "        h = ln.register_forward_hook(make_hook(name))\n",
    "        handles.append(h)\n",
    "    model.eval()\n",
    "    seen = 0\n",
    "    for ex in tqdm(dataset, total=min(num_samples, len(dataset)), desc=\"Calibrating\"):\n",
    "        if seen >= num_samples:\n",
    "            break\n",
    "        text = ex[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=seq_len,\n",
    "        )\n",
    "        enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "        _ = model(**enc)\n",
    "        seen += 1\n",
    "    for h in handles:\n",
    "        h.remove()\n",
    "    act_scales = {k: v.cpu() for k, v in act_scales.items()}\n",
    "    return act_scales\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Part 2: Perplexity Evaluator\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, dataset, tokenizer, device, n_samples=128):\n",
    "        texts = []\n",
    "        for ex in dataset:\n",
    "            if len(texts) >= n_samples:\n",
    "                break\n",
    "            t = ex[\"text\"].strip()\n",
    "            if t:\n",
    "                texts.append(t)\n",
    "        enc = tokenizer(\"\\n\\n\".join(texts), return_tensors=\"pt\")\n",
    "        self.input_ids = enc[\"input_ids\"][0].to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, model, seq_len=2048):\n",
    "        nlls = []\n",
    "        ids = self.input_ids\n",
    "        for i in tqdm(\n",
    "            range(0, ids.size(0) - 1, seq_len),\n",
    "            desc=\"Evaluating PPL\",\n",
    "        ):\n",
    "            input_ids = ids[i : i + seq_len].unsqueeze(0)\n",
    "            labels = input_ids.clone()\n",
    "            outputs = model(\n",
    "                input_ids.to(self.device),\n",
    "                labels=labels.to(self.device),\n",
    "            )\n",
    "            nlls.append(outputs.loss.item())\n",
    "        mean_nll = sum(nlls) / len(nlls)\n",
    "        ppl = math.exp(mean_nll)\n",
    "        return ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VS5D_NngG2U2",
   "metadata": {
    "id": "VS5D_NngG2U2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m config_name, config \u001b[38;5;129;01min\u001b[39;00m configs.items():\n\u001b[32m    106\u001b[39m     model, tokenizer = load_model_and_tokenizer(model_id)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     results[model_name][config_name] = run_experiment(\n\u001b[32m    109\u001b[39m         model_id, config, calibration_dataset, eval_dataset, model, tokenizer\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    111\u001b[39m     torch.cuda.empty_cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/eml/p4/.venv/lib/python3.13/site-packages/accelerate/big_modeling.py:462\u001b[39m, in \u001b[36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.parameters():\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m param.device == torch.device(\u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "### Cell 8: Main Experiment - Apply SmoothQuant and Evaluate\n",
    "\n",
    "import gc\n",
    "\n",
    "def run_experiment(model_id, quant_config, calibration_ds, evaluation_ds, model=None, tokenizer=None):\n",
    "    print(f\"\\n=== Experiment: {model_id} | {quant_config} ===\")\n",
    "    if model is None or tokenizer is None:\n",
    "        model, tokenizer = load_model_and_tokenizer(model_id)\n",
    "\n",
    "    w_bits = quant_config.get(\"w_bits\", None)\n",
    "    a_bits = quant_config.get(\"a_bits\", None)\n",
    "    smooth = quant_config.get(\"smooth\", False)\n",
    "    alpha = quant_config.get(\"alpha\", 0.5)\n",
    "\n",
    "    if w_bits is not None and a_bits is not None and smooth:\n",
    "        act_scales = get_act_scales(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            calibration_ds,\n",
    "            num_samples=quant_config.get(\"calib_samples\", 256),\n",
    "            seq_len=quant_config.get(\"calib_seq_len\", 512),\n",
    "        )\n",
    "        model = smooth_model(model, act_scales, alpha=alpha)\n",
    "\n",
    "    if w_bits is not None and a_bits is not None:\n",
    "        model = quantize_model(model, w_bits=w_bits, a_bits=a_bits)\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        evaluation_ds,\n",
    "        tokenizer,\n",
    "        DEVICE,\n",
    "        n_samples=quant_config.get(\"eval_samples\", 128),\n",
    "    )\n",
    "    ppl = evaluator.evaluate(model, seq_len=quant_config.get(\"eval_seq_len\", 2048))\n",
    "    print(f\"Perplexity: {ppl:.3f}\")\n",
    "    return ppl\n",
    "\n",
    "# --- Experiment Configurations ---\n",
    "experiment_configs = {\n",
    "    \"Llama-3-8B\": {\n",
    "        \"bf16_baseline\": {\n",
    "            \"w_bits\": None,\n",
    "            \"a_bits\": None,\n",
    "            \"smooth\": False,\n",
    "            \"eval_samples\": 128,\n",
    "        },\n",
    "        \"naive_W8A8\": {\n",
    "            \"w_bits\": 8,\n",
    "            \"a_bits\": 8,\n",
    "            \"smooth\": False,\n",
    "            \"calib_samples\": 256,\n",
    "            \"eval_samples\": 128,\n",
    "        },\n",
    "        \"W8A8_SmoothQuant_alpha0.5\": {\n",
    "            \"w_bits\": 8,\n",
    "            \"a_bits\": 8,\n",
    "            \"smooth\": True,\n",
    "            \"alpha\": 0.5,\n",
    "            \"calib_samples\": 256,\n",
    "            \"eval_samples\": 128,\n",
    "        },\n",
    "    },\n",
    "    \"Llama-2-7B\": {\n",
    "        \"bf16_baseline\": {\n",
    "            \"w_bits\": None,\n",
    "            \"a_bits\": None,\n",
    "            \"smooth\": False,\n",
    "            \"eval_samples\": 128,\n",
    "        },\n",
    "        \"naive_W8A8\": {\n",
    "            \"w_bits\": 8,\n",
    "            \"a_bits\": 8,\n",
    "            \"smooth\": False,\n",
    "            \"calib_samples\": 256,\n",
    "            \"eval_samples\": 128,\n",
    "        },\n",
    "        \"W8A8_SmoothQuant_alpha0.5\": {\n",
    "            \"w_bits\": 8,\n",
    "            \"a_bits\": 8,\n",
    "            \"smooth\": True,\n",
    "            \"alpha\": 0.5,\n",
    "            \"calib_samples\": 256,\n",
    "            \"eval_samples\": 128,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "MODEL_MAPPING = {\n",
    "    \"Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"Llama-2-7B\": \"meta-llama/Llama-2-7b-hf\",\n",
    "}\n",
    "\n",
    "# --- Run all experiments and collect results ---\n",
    "results = {}\n",
    "for model_name, configs in experiment_configs.items():\n",
    "    model_id = MODEL_MAPPING[model_name]\n",
    "    results[model_name] = {}\n",
    "    for config_name, config in configs.items():\n",
    "        model, tokenizer = load_model_and_tokenizer(model_id)\n",
    "        results[model_name][config_name] = run_experiment(\n",
    "            model_id, config, calibration_dataset, eval_dataset, model, tokenizer\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ebe8b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  13906 MiB |  23009 MiB | 158286 MiB | 144379 MiB |\n",
      "|       from large pool |  13906 MiB |  23008 MiB | 137009 MiB | 123103 MiB |\n",
      "|       from small pool |      0 MiB |     39 MiB |  21276 MiB |  21276 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  13906 MiB |  23009 MiB | 158286 MiB | 144379 MiB |\n",
      "|       from large pool |  13906 MiB |  23008 MiB | 137009 MiB | 123103 MiB |\n",
      "|       from small pool |      0 MiB |     39 MiB |  21276 MiB |  21276 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  13906 MiB |  23009 MiB | 157977 MiB | 144070 MiB |\n",
      "|       from large pool |  13906 MiB |  23008 MiB | 136706 MiB | 122799 MiB |\n",
      "|       from small pool |      0 MiB |     39 MiB |  21271 MiB |  21270 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  19012 MiB |  23132 MiB |  44670 MiB |  25658 MiB |\n",
      "|       from large pool |  19010 MiB |  23128 MiB |  44624 MiB |  25614 MiB |\n",
      "|       from small pool |      2 MiB |     42 MiB |     46 MiB |     44 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   5105 MiB |  15101 MiB | 129848 MiB | 124743 MiB |\n",
      "|       from large pool |   5103 MiB |  15099 MiB | 108024 MiB | 102920 MiB |\n",
      "|       from small pool |      1 MiB |     22 MiB |  21824 MiB |  21822 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     282    |     489    |   93409    |   93127    |\n",
      "|       from large pool |     219    |     391    |   14019    |   13800    |\n",
      "|       from small pool |      63    |     207    |   79390    |   79327    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     282    |     489    |   93409    |   93127    |\n",
      "|       from large pool |     219    |     391    |   14019    |   13800    |\n",
      "|       from small pool |      63    |     207    |   79390    |   79327    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       5    |      33    |     219    |     214    |\n",
      "|       from large pool |       4    |      25    |     196    |     192    |\n",
      "|       from small pool |       1    |      21    |      23    |      22    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |      42    |   47604    |   47599    |\n",
      "|       from large pool |       4    |      10    |    5769    |    5765    |\n",
      "|       from small pool |       1    |      41    |   41835    |   41834    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lXOIPk4XG2U3",
   "metadata": {
    "id": "lXOIPk4XG2U3"
   },
   "outputs": [],
   "source": [
    "### Cell 9: Results Summary and Analysis\n",
    "\n",
    "# --- 1. Format results as a table for easy comparison ---\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\" \" * 15 + \"Experiment Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "# TODO: Format and display results (e.g., Markdown table).\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Persist results if needed (e.g., CSV export).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7-uqUqFGG2U3",
   "metadata": {
    "id": "7-uqUqFGG2U3"
   },
   "outputs": [],
   "source": [
    "### Cell 10: List All Generated Artifacts\n",
    "print(\"Task 3 complete. Generated artifacts:\")\n",
    "if os.path.isdir(FIGURES_DIR):\n",
    "    print(\"Figures:\")\n",
    "    # TODO: List figure artifacts that were generated.\n",
    "if os.path.isdir(RESULTS_DIR):\n",
    "    print(\"Results:\")\n",
    "    # TODO: List result artifacts that were generated.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
