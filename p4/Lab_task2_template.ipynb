{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b276258d",
   "metadata": {
    "id": "b276258d"
   },
   "source": [
    "# **Task 2: Visualizing Attention and Analyzing the “Attention Sink” Phenomenon**\n",
    "\n",
    "## **Overview**\n",
    "Explores **Transformer self-attention** by capturing, visualizing, and quantifying attention maps, then simulates how **StreamLLM** leverages the *“attention sink”* to enable long-sequence generation with fixed memory.  \n",
    "The notebook adds `output_attentions=True` for introspection, builds a hook-based **Attention Catcher**, measures sink attention across layers, and contrasts standard **KV caching** with a **StreamLLM-style cache** that trims middle tokens while preserving sinks and recency.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Load the Model in Investigation Mode (Cell 4)**\n",
    "\n",
    "- **Enable attention outputs:**  \n",
    "  Load the model with  \n",
    "  `AutoModelForCausalLM.from_pretrained(..., output_attentions=True)`  \n",
    "  so each forward pass returns per-layer attention tensors alongside logits.\n",
    "\n",
    "- **Tokenization setup:**  \n",
    "  Initialize the matching `AutoTokenizer` and standard generation configs used throughout experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Build the “Attention Catcher” Toolkit (Cell 5)**\n",
    "\n",
    "- **Global storage:**  \n",
    "  `attention_maps_storage` keeps captured attention tensors keyed by layer/module identifiers.\n",
    "\n",
    "- **Hook factory:**  \n",
    "  `get_attention_hook` returns a forward hook that extracts `attn_weights` during the pass and stores them in `attention_maps_storage`.\n",
    "\n",
    "- **Hook registrar:**  \n",
    "  `register_attention_hooks(model, layers=...)` attaches the hook to each chosen self-attention module (single layer, a subset, or all layers).\n",
    "\n",
    "- **Visualization:**  \n",
    "  `plot_attention_maps` retrieves saved maps, aggregates across heads (e.g., mean over heads), and renders **attention heatmaps** for inspection.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Experiment 1 — Attention Patterns Across Inputs (Cell 6)**\n",
    "\n",
    "- **Inputs:**  \n",
    "  Define `INPUT_TEXTS` with both meaningful sentences and repetitive “dummy” strings of varying lengths.\n",
    "\n",
    "- **Hook installation:**  \n",
    "  Call `register_attention_hooks` for selected layers; clear `attention_maps_storage` before each run.\n",
    "\n",
    "- **Single forward pass:**  \n",
    "  Tokenize each input and run a forward pass to populate attention storage via hooks.\n",
    "\n",
    "- **Heatmaps:**  \n",
    "  Use `plot_attention_maps` to visualize per-layer attention, aggregated across heads, for side-by-side comparison across inputs.\n",
    "\n",
    "- **Cleanup:**  \n",
    "  Remove all hooks after the experiment to avoid extra overhead later.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Experiment 2 — Quantifying the Attention Sink (Cell 7)**\n",
    "\n",
    "- **Full-depth monitoring:**  \n",
    "  Register hooks on every layer to capture a complete attention profile across the model.\n",
    "\n",
    "- **Sink metric:**  \n",
    "  For each input and layer, compute the fraction of attention mass directed to the first `SINK_TOKEN_WINDOW` tokens, averaging across heads and query positions.\n",
    "\n",
    "- **Trends by layer:**  \n",
    "  Store results and plot layer-wise curves  \n",
    "  *(x-axis: layer ID; y-axis: sink attention %)*  \n",
    "  with separate lines per input type to reveal consistent sink patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5 (Advanced): StreamLLM Simulation and Memory Advantage (Cells 8–9)**\n",
    "\n",
    "- **Positional shift attention:**  \n",
    "  `llama_pos_shift_attention_forward` modifies the attention forward path to dynamically adjust positional encodings (e.g., RoPE phases) when intermediate tokens are evicted, preserving correct relative positions among the remaining tokens.\n",
    "\n",
    "- **KV cache manager:**  \n",
    "  `streamingllm_kv` tracks and trims the KV cache by discarding middle tokens once capacity is exceeded, keeping only early “sink” tokens and the most recent tokens.\n",
    "\n",
    "- **Baseline vs. StreamLLM:**  \n",
    "  - `run_baseline_experiment`: Standard generation where KV cache grows linearly with sequence length; log memory usage over steps.  \n",
    "  - `run_streamllm_experiment`: Generation with `streamingllm_kv` trimming after each step; log memory usage for comparison.\n",
    "\n",
    "- **Analysis:**  \n",
    "  Plot both memory curves against generated tokens to show linear growth (**baseline**) vs. plateau (**StreamLLM-style trimming**), illustrating fixed-memory long-context generation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Results and Takeaways**\n",
    "\n",
    "- **Memory efficiency:**  \n",
    "  StreamLLM-style KV trimming flattens memory growth, enabling sustained generation without exhausting memory.\n",
    "\n",
    "- **Output quality:**  \n",
    "  Standard full-cache generation degrades (e.g., incoherent characters) far beyond training lengths, while StreamLLM maintains more coherent outputs under extended contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbef0a65",
   "metadata": {
    "id": "bbef0a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.6\n",
      "PyTorch version: 2.9.0+cu126\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "Environment initialised.\n"
     ]
    }
   ],
   "source": [
    "### Cell 2: Environment Setup and Dependency Installation\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.stopping_criteria import StoppingCriteria\n",
    "from transformers.utils import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "RESULTS_DIR = \"./results\"\n",
    "FIGURES_DIR = \"./figures\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    # print GPU diagnostics here\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    # print CPU-only notice\n",
    "\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch RNGs for reproducible lab runs.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def require_gpu(task: str) -> None:\n",
    "    \"\"\"Raise a descriptive error if a GPU is required but not available.\"\"\"\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        return\n",
    "    raise RuntimeError(f\"Skipped: {task} requires GPU. Please set DEVICE to 'cuda'.\")\n",
    "\n",
    "set_seed()\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "print(\"Environment initialised.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "QN0aHmATG2RB",
   "metadata": {
    "id": "QN0aHmATG2RB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hugging Face login successful!\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 3: Hugging Face Login\n",
    "from huggingface_hub import login, HfFolder\n",
    "from getpass import getpass\n",
    "\n",
    "# Check if a Hugging Face token is already set in the environment.\n",
    "if not os.getenv(\"HUGGING_FACE_HUB_TOKEN\"):\n",
    "    try:\n",
    "        # Prompt user for Hugging Face access token if not found.\n",
    "        hf_token = getpass(\"Please enter your Hugging Face access token: \")\n",
    "        login(token=hf_token, add_to_git_credential=True)\n",
    "        print(\"   Hugging Face login successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}. Model loading may fail later.\")\n",
    "else:\n",
    "    print(\"   Hugging Face token detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "paVt6M9LG2RB",
   "metadata": {
    "id": "paVt6M9LG2RB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e22da66ef046be820a3adac6e29d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a445048ee4488daf3a984fb3811268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18da8049f0564433a7ea3b6d525cabf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a55f75205d43979fb0aa7c698ea677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493fb894f8f44524b164570c763576cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e3d172922f4319a3ae12ec1ce4c76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7982ff1b407400ba60d0e09edc8a70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e242b674ff5c4cbcae956db2e5dba95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### Cell 4: Load Model and Tokenizer\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "FALLBACK_MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model: Optional[AutoModelForCausalLM] = None\n",
    "tokenizer: Optional[AutoTokenizer] = None\n",
    "\n",
    "candidates = [MODEL_ID, FALLBACK_MODEL_ID]\n",
    "\n",
    "for candidate in candidates:\n",
    "    model = AutoModelForCausalLM.from_pretrained(candidate)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(candidate)\n",
    "    if model is not None and tokenizer is not None:\n",
    "        break\n",
    "\n",
    "if model is None or tokenizer is None:\n",
    "    raise RuntimeError(\"Failed to load any model/tokenizer candidates.\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded: {model.__class__.__name__} ({model.config.model_type})\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__} ({tokenizer.name_or_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed2d25",
   "metadata": {
    "id": "a3ed2d25"
   },
   "outputs": [],
   "source": [
    "### Cell 5: Core Functions for Attention Extraction and Visualization\n",
    "\n",
    "# Global storage for attention maps, keyed by layer name\n",
    "attention_maps_storage = {}\n",
    "\n",
    "def get_attention_hook(layer_name):\n",
    "    \"\"\"Return a forward hook function that stores attention weights for the given layer.\"\"\"\n",
    "    # TODO: capture attention tensors and store them in attention_maps_storage\n",
    "    ...\n",
    "\n",
    "def register_attention_hooks(model, layers_to_hook):\n",
    "    \"\"\"Register forward hooks on attention modules for the requested layers.\"\"\"\n",
    "    hooks = []\n",
    "    # TODO: locate attention modules (e.g., LlamaAttention) and register hooks\n",
    "    # hook_handle = attn_module.register_forward_hook(get_attention_hook(...))\n",
    "    # hooks.append(hook_handle)\n",
    "    return hooks\n",
    "\n",
    "def plot_attention_maps(attention_maps, tokens, layers_to_plot, file_prefix):\n",
    "    \"\"\"Visualise attention maps for selected layers and save the figure.\"\"\"\n",
    "    # TODO: aggregate attention across heads, configure subplots, and render heatmaps\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba1dbb",
   "metadata": {
    "id": "10ba1dbb"
   },
   "outputs": [],
   "source": [
    "### Cell 6: Experiment - Visualize Attention Maps for Different Inputs\n",
    "\n",
    "# --- Configurable Section ---\n",
    "INPUT_TEXTS = {\n",
    "    \"short_dummy\": \"...\",  # TODO: provide dummy prompt\n",
    "    \"short_meaningful\": \"...\",  # TODO: provide meaningful prompt\n",
    "    \"medium_dummy\": \"...\",\n",
    "    \"medium_meaningful\": \"...\",\n",
    "    \"long_dummy\": \"...\",\n",
    "    \"long_meaningful\": \"...\",\n",
    "}\n",
    "\n",
    "LAYERS_TO_VISUALIZE = [...]  # TODO: select representative layer indices\n",
    "if \"1b\" in MODEL_ID.lower():\n",
    "    LAYERS_TO_VISUALIZE = [...]  # TODO: adjust layers for smaller models\n",
    "# ---\n",
    "\n",
    "hooks = []  # TODO: register attention hooks for the selected layers\n",
    "# hooks = register_attention_hooks(model, LAYERS_TO_VISUALIZE)\n",
    "\n",
    "for name, text in INPUT_TEXTS.items():\n",
    "    print(f\"\\n--- Processing input: {name} ---\")\n",
    "    attention_maps_storage.clear()\n",
    "    # TODO: tokenize text, run the model with output_attentions=True, and collect attention maps\n",
    "    # inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(DEVICE)\n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model(**inputs, output_attentions=True)\n",
    "    # tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    # plot_attention_maps(attention_maps_storage, tokens, LAYERS_TO_VISUALIZE, name)\n",
    "    pass\n",
    "\n",
    "# TODO: remove hooks after visualization to avoid memory leaks\n",
    "# for handle in hooks:\n",
    "#     handle.remove()\n",
    "print(\"\\n   All attention maps for the provided inputs have been generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6SPZKujG2RD",
   "metadata": {
    "id": "t6SPZKujG2RD"
   },
   "outputs": [],
   "source": [
    "### Cell 7: Experiment - Attention Sink Phenomenon Analysis\n",
    "\n",
    "print(\"\\n--- Starting Experiment: Attention Sink Phenomenon Analysis ---\")\n",
    "# --- Configurable Section ---\n",
    "SINK_TOKEN_WINDOW = ...  # TODO: choose number of initial tokens treated as sink tokens\n",
    "# ---\n",
    "\n",
    "sink_analysis_results = []\n",
    "hooks = []  # TODO: register attention hooks across all layers\n",
    "# hooks = register_attention_hooks(model, range(model.config.num_hidden_layers))\n",
    "\n",
    "for name, text in INPUT_TEXTS.items():\n",
    "    attention_maps_storage.clear()\n",
    "    # TODO: tokenize text and run the model to capture attention maps\n",
    "    # inputs = tokenizer(text, return_tensors=\"pt\", max_length=256).to(DEVICE)\n",
    "    # with torch.no_grad():\n",
    "    #     outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    for layer_idx in range(model.config.num_hidden_layers):\n",
    "        layer_name = f\"layer_{layer_idx}\"\n",
    "        if layer_name in attention_maps_storage:\n",
    "            # TODO: compute sink attention statistics and append to sink_analysis_results\n",
    "            # attn_map = attention_maps_storage[layer_name][0].mean(dim=0)\n",
    "            # sink_attention_strength = attn_map[SINK_TOKEN_WINDOW:, :SINK_TOKEN_WINDOW].sum().item()\n",
    "            # total_attention = attn_map[SINK_TOKEN_WINDOW:, :].sum().item()\n",
    "            # sink_percentage = (sink_attention_strength / total_attention) * 100 if total_attention > 0 else 0\n",
    "            # sink_analysis_results.append({\"Input Type\": name, \"Layer ID\": layer_idx, \"Sink Attention (%)\": sink_percentage})\n",
    "            pass\n",
    "\n",
    "# TODO: remove hooks after analysis\n",
    "# for handle in hooks:\n",
    "#     handle.remove()\n",
    "\n",
    "# TODO: convert sink_analysis_results into a DataFrame and save to disk\n",
    "# df_sink = pd.DataFrame(sink_analysis_results)\n",
    "# df_sink.to_csv(\"./results/task2_attention_sink_analysis.csv\", index=False)\n",
    "\n",
    "# TODO: plot sink attention percentage vs. layer depth for each input type\n",
    "# plt.figure(...)\n",
    "# sns.lineplot(...)\n",
    "# plt.savefig(\"./figures/task2_attention_sink_analysis.png\", dpi=300)\n",
    "\n",
    "print(\"\\n--- Attention Sink Analysis Results ---\")\n",
    "# TODO: summarise sink attention statistics and display the plot\n",
    "# print(df_sink.groupby(\"Input Type\")[\"Sink Attention (%)\"].mean().reset_index())\n",
    "# plt.show()\n",
    "\n",
    "\"\"\"\n",
    "#### Attention Sink Phenomenon Analysis\n",
    "\n",
    "**Observed Phenomena:**\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e4671",
   "metadata": {
    "id": "9b4e4671"
   },
   "outputs": [],
   "source": [
    "### Cell 8: Bonus Experiment: Modify Standard Attention to StreamingLLM Attention (Task 2 Step 4)\n",
    "# TODO: import required modules for custom attention (transformers attention utilities, logging, torch.nn, types)\n",
    "# from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n",
    "# from transformers.models.llama.modeling_llama import LlamaAttention, rotate_half, repeat_kv\n",
    "# from transformers.utils import logging\n",
    "# import torch.nn as nn\n",
    "# import types\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements the standard (eager) attention forward pass.\n",
    "    \"\"\"\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    attn_weights = nn.functional.softmax(\n",
    "        attn_weights, dim=-1, dtype=torch.float32\n",
    "    ).to(query.dtype)\n",
    "    attn_weights = nn.functional.dropout(\n",
    "        attn_weights, p=dropout, training=module.training\n",
    "    )\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "\n",
    "    return attn_output, attn_weights\n",
    "\n",
    "def apply_rotary_pos_emb_single(x, cos, sin, position_ids):\n",
    "    \"\"\"\n",
    "    Applies rotary positional embedding to a single tensor.\n",
    "    \"\"\"\n",
    "    # Remove singleton dimensions for broadcasting\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    x_embed = (x * cos) + (rotate_half(x) * sin)\n",
    "    return x_embed\n",
    "\n",
    "def apply_rotary_pos_emb_q(q, cos, sin, unsqueeze_dim=1):\n",
    "    \"\"\"\n",
    "    Applies rotary positional embedding to the query tensor.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    return q_embed\n",
    "\n",
    "\n",
    "# TODO: refer to https://github.com/mit-han-lab/streaming-llm/blob/main/streaming_llm/pos_shift/modify_llama.py\n",
    "# modify to fit llama3 architecture\n",
    "def llama_pos_shift_attention_forward(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position=None, **kwargs):\n",
    "    \"\"\"Modified LLaMA attention forward pass with position shifting for StreamLLM.\"\"\"\n",
    "    # TODO: project QKV, update caches, apply rotary embeddings, and compute attention outputs\n",
    "    ...\n",
    "\n",
    "def enable_llama_pos_shift_attention(model):\n",
    "    \"\"\"Replace standard LlamaAttention.forward methods with the position-shifted variant.\"\"\"\n",
    "    # TODO: recursively locate LlamaAttention modules and bind llama_pos_shift_attention_forward\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a2eb7",
   "metadata": {
    "id": "0e2a2eb7"
   },
   "outputs": [],
   "source": [
    "### Cell 9: Bonus Experiment: Investigate StreamLLM's Impact on Long-Sequence Memory Usage\n",
    "# TODO: import tqdm for progress visualisation\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n--- Starting Bonus Experiment: Simulating StreamLLM Memory Impact ---\")\n",
    "\n",
    "# --- Configurable Section ---\n",
    "BONUS_PROMPT = \"...\"  # TODO: provide long-form prompt\n",
    "BONUS_GENERATION_LENGTH = ...  # TODO: choose number of tokens to generate\n",
    "BONUS_SAMPLING_INTERVAL = ...  # TODO: sampling interval for memory measurements\n",
    "STREAMLLM_CACHE_SIZE = ...  # TODO: number of sink tokens to retain\n",
    "STREAMLLM_RECENT_SIZE = ...  # TODO: number of most recent tokens to retain\n",
    "# ---\n",
    "\n",
    "def run_baseline_experiment(model, tokenizer, prompt, generation_length, sampling_interval, device):\n",
    "    \"\"\"Run baseline generation with the standard KV cache while logging memory usage.\"\"\"\n",
    "    # TODO: implement generation loop without cache eviction and record GPU memory\n",
    "    ...\n",
    "\n",
    "class streamingllm_kv:\n",
    "    \"\"\"Implement StreamLLM-style KV cache eviction (retain sink + recent tokens).\"\"\"\n",
    "    def __init__(self, start_size, recent_size, past_key_values):\n",
    "        # TODO: store configuration for cache trimming\n",
    "        ...\n",
    "\n",
    "    def __call__(self, kv_cache):\n",
    "        \"\"\"Trim the KV cache according to the StreamLLM policy.\"\"\"\n",
    "        # TODO: drop middle tokens while retaining sink and recent tokens\n",
    "        ...\n",
    "\n",
    "def run_streamllm_experiment(model, tokenizer, prompt, generation_length, sampling_interval, sink_size, recent_size, device):\n",
    "    \"\"\"Run generation with StreamLLM cache eviction and record memory usage.\"\"\"\n",
    "    # TODO: enable modified attention, apply streaming cache policy, and log memory\n",
    "    ...\n",
    "\n",
    "# =================================================================================\n",
    "# Main Execution Flow\n",
    "# =================================================================================\n",
    "\n",
    "# TODO: prepare chat-formatted prompt and run both baseline and StreamLLM experiments\n",
    "# messages = [...]\n",
    "# BONUS_PROMPT = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# memory_baseline, last100words_baseline = run_baseline_experiment(...)\n",
    "# memory_streamllm, last100words_streamllm = run_streamllm_experiment(...)\n",
    "\n",
    "# TODO: collate memory usage results, save CSV artifacts, and plot comparisons\n",
    "# df_mem_compare = pd.concat([...])\n",
    "# df_mem_compare.to_csv(\"./results/task2_bonus_memory_comparison.csv\", index=False)\n",
    "# sns.lineplot(...)\n",
    "# plt.savefig(\"./figures/task2_bonus_memory_comparison.png\", dpi=300)\n",
    "\n",
    "# TODO: decode and print the final segments from each generation for qualitative comparison\n",
    "# print(tokenizer.decode(last100words_baseline))\n",
    "# print(tokenizer.decode(last100words_streamllm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90324274",
   "metadata": {
    "id": "90324274"
   },
   "outputs": [],
   "source": [
    "### Cell 10: List all generated artifacts for Task 2\n",
    "print(\"Task 2 complete. Generated artifacts:\")\n",
    "\n",
    "# TODO: iterate over output directories and list generated files\n",
    "# if os.path.isdir(FIGURES_DIR):\n",
    "#     ...\n",
    "# if os.path.isdir(RESULTS_DIR):\n",
    "#     ...\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
