{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23c7ab1",
   "metadata": {
    "id": "a23c7ab1"
   },
   "source": [
    "# **Task 1: LLM Fundamentals & Generation Analysis**\n",
    "\n",
    "## **Overview**\n",
    "This task provides a **hands-on introduction** to the fundamentals of large language models (LLMs) using the **Llama 3-8B Instruct** model.  \n",
    "It walks through model loading, text generation experiments, and key performance analyses, focusing on **latency**, **diversity**, and **memory usage**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Environment Setup (Cell 2)**\n",
    "\n",
    "- **Objective:**  \n",
    "  Create a clean and reproducible environment for running all experiments.\n",
    "\n",
    "- **Actions:**\n",
    "  - **Import Libraries:**  \n",
    "    Load all required libraries for model handling, generation, and visualization.  \n",
    "  - **Configure Environment:**  \n",
    "    Define a `set_seed` function to ensure reproducibility by fixing random seeds.  \n",
    "    Verify that a **GPU** is available for efficient computation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Model Loading (Cell 4)**\n",
    "\n",
    "- **Objective:**  \n",
    "  Load the target **Llama 3-8B Instruct** model and prepare it for inference.\n",
    "\n",
    "- **Actions:**\n",
    "  - **Select Model:**  \n",
    "    Set `MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"`.  \n",
    "  - **Load with Fallback:**  \n",
    "    Use a `try...except` block to attempt loading the main model.  \n",
    "    If it fails (e.g., due to limited memory), load a smaller fallback model (`FALLBACK_MODEL_ID`) to ensure the notebook remains executable.  \n",
    "  - **Model Summary:**  \n",
    "    Print key information about the loaded model, including size and configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Experiment 1 — Effect of Temperature on Diversity (Cell 5)**\n",
    "\n",
    "- **Objective:**  \n",
    "  Understand how the **temperature** hyperparameter influences the **randomness and diversity** of generated text.\n",
    "\n",
    "- **Actions:**\n",
    "  - **Set Parameters:**  \n",
    "    Define a fixed input `PROMPT` and a list of `TEMPERATURES` to test.  \n",
    "  - **Generate Samples:**  \n",
    "    Loop through each temperature value and generate multiple text samples using  \n",
    "    `model.generate(..., do_sample=True)` so that temperature actually affects randomness.  \n",
    "  - **Record Results:**  \n",
    "    Save all generated outputs for side-by-side comparison and qualitative analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Experiment 2 — Input Length vs. Prefill Latency (Cell 6)**\n",
    "\n",
    "- **Objective:**  \n",
    "  Demonstrate that **longer input sequences** lead to **higher prefill latency** before generation begins.\n",
    "\n",
    "- **Actions:**\n",
    "  - **Define Input Sizes:**  \n",
    "    Create a list of token lengths (`INPUT_LENGTHS`) to test various input sizes.  \n",
    "  - **Precise Timing:**  \n",
    "    Use `torch.cuda.Event` for high-precision GPU time measurement.  \n",
    "  - **Measure Prefill Stage:**  \n",
    "    For each input length, perform warm-up runs, then measure time for generating one token (`max_new_tokens=1`) to isolate prefill cost.  \n",
    "  - **Visualize:**  \n",
    "    Plot a line graph showing **input length (x-axis)** vs. **average latency (y-axis)**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Experiment 3 — Real-Time Memory Tracking During Generation (Cell 8)**\n",
    "\n",
    "- **Objective:**  \n",
    "  Track and visualize **GPU memory usage** step-by-step as tokens are generated, illustrating the growth of the **KV Cache**.\n",
    "\n",
    "- **Actions:**\n",
    "  - **Design a “Memory Hook”:**  \n",
    "    Implement a custom class `MemoryUsageCallback` inheriting from `transformers.StoppingCriteria`.  \n",
    "    Its `__call__` method records current GPU memory after each token generation, returning `False` to continue.  \n",
    "  - **Run Monitored Generation:**  \n",
    "    After GPU warm-up, create an instance of the callback and pass it to `model.generate()` via `stopping_criteria`.  \n",
    "  - **Analyze and Plot:**  \n",
    "    Convert recorded memory data into a DataFrame and plot the **memory usage curve** over generation steps.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Verifying the Space Complexity of KV Cache (Cell 9)**\n",
    "\n",
    "- **Objective:**  \n",
    "  Quantitatively confirm that memory usage grows **linearly** with sequence length — proving **O(L)** space complexity for the KV Cache.\n",
    "\n",
    "- **Actions:**\n",
    "  - **Linear Regression:**  \n",
    "    Apply `scipy.stats.linregress` to fit a linear model between the number of generated tokens and memory increase.  \n",
    "  - **Interpret Results:**  \n",
    "    - The **slope** indicates average memory growth (in MB) per generated token.  \n",
    "    - The **R² value** close to 1 (e.g., > 0.99) validates a strong linear correlation, confirming the **O(L)** relationship.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: Summary and Analysis (Cell 10)**\n",
    "\n",
    "- **Objective:**  \n",
    "  Summarize all experimental findings and explain the underlying principles observed.\n",
    "\n",
    "- **Actions:**\n",
    "  - **Write Analysis:**  \n",
    "    Discuss results and plots from all experiments, highlighting:  \n",
    "    - The trade-off between temperature and diversity.  \n",
    "    - The linear scaling of prefill latency with input length.  \n",
    "    - The direct, linear memory growth due to KV caching.  \n",
    "  - Synthesize these insights into a clear, data-driven conclusion about **LLM generation efficiency** and **scalability**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b13aaf1",
   "metadata": {
    "id": "1b13aaf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.8\n",
      "PyTorch version: 2.9.1+cu128\n",
      "Python version: 3.13.3 (main, Apr  9 2025, 04:03:52) [Clang 20.1.0 ]\n",
      "Environment initialised.\n"
     ]
    }
   ],
   "source": [
    "### Cell 2: Environment Setup and Dependency Installation\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.stopping_criteria import StoppingCriteria\n",
    "from transformers.utils import logging\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RESULTS_DIR = \"./results\"\n",
    "FIGURES_DIR = \"./figures\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    # print GPU diagnostics here\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    # print CPU-only notice\n",
    "\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch RNGs for reproducible lab runs.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def require_gpu(task: str) -> None:\n",
    "    \"\"\"Raise a descriptive error if a GPU is required but not available.\"\"\"\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        return\n",
    "    raise RuntimeError(f\"Skipped: {task} requires GPU. Please set DEVICE to 'cuda'.\")\n",
    "\n",
    "set_seed()\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "plt.rcParams.update({\"font.size\": 14})\n",
    "print(\"Environment initialised.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44365f03",
   "metadata": {
    "id": "44365f03"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "   Hugging Face login successful!\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 3: Hugging Face Login\n",
    "from huggingface_hub import login, HfFolder\n",
    "from getpass import getpass\n",
    "\n",
    "# Check if a Hugging Face token is already set in the environment.\n",
    "if not os.getenv(\"HUGGING_FACE_HUB_TOKEN\"):\n",
    "    try:\n",
    "        # Prompt user for Hugging Face access token if not found.\n",
    "        hf_token = getpass(\"Please enter your Hugging Face access token: \")\n",
    "        login(token=hf_token, add_to_git_credential=True)\n",
    "        print(\"   Hugging Face login successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Login failed: {e}. Model loading may fail later.\")\n",
    "else:\n",
    "    print(\"   Hugging Face token detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9885e8",
   "metadata": {
    "id": "be9885e8"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "meta-llama/Llama-3-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3-8B-Instruct/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1114\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1655\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1652\u001b[39m ):\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/file_download.py:307\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    306\u001b[39m response = http_backoff(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:452\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    443\u001b[39m     message = (\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 404 Client Error. (Request ID: Root=1-692be2ad-5b4d83493474b8f00183a87a;f60098a5-8328-4d57-9750-ffedfd3ec7c9)\n\nRepository Not Found for url: https://huggingface.co/meta-llama/Llama-3-8B-Instruct/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m candidates = [MODEL_ID, FALLBACK_MODEL_ID]\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m candidate \u001b[38;5;129;01min\u001b[39;00m candidates:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m.to(DEVICE)\n\u001b[32m     12\u001b[39m     tokenizer = AutoTokenizer.from_pretrained(candidate).to(DEVICE)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:508\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m    507\u001b[39m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m         resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m         commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/School/CS4803-EML/p4/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:511\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    509\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    512\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    519\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    520\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    521\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: meta-llama/Llama-3-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "### Cell 4: Load Model and Tokenizer\n",
    "MODEL_ID = \"meta-llama/Llama-3-8B-Instruct\"\n",
    "FALLBACK_MODEL_ID = \"meta-llama/Llama-3-8B-Instruct\"\n",
    "\n",
    "model: Optional[AutoModelForCausalLM] = None\n",
    "tokenizer: Optional[AutoTokenizer] = None\n",
    "\n",
    "candidates = [MODEL_ID, FALLBACK_MODEL_ID]\n",
    "\n",
    "for candidate in candidates:\n",
    "    model = AutoModelForCausalLM.from_pretrained(candidate).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(candidate).to(DEVICE)\n",
    "    if model is not None and tokenizer is not None:\n",
    "        break\n",
    "\n",
    "if model is None or tokenizer is None:\n",
    "    raise RuntimeError(\"Failed to load any model/tokenizer candidates.\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded: {model.__class__.__name__} ({model.config.model_type})\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__} ({tokenizer.name_or_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a426393",
   "metadata": {
    "id": "0a426393"
   },
   "outputs": [],
   "source": [
    "### Cell 5: Experiment 1 - Effect of Temperature on Generation Diversity\n",
    "print(\"--- Experiment 1: Temperature sweep ---\")\n",
    "\n",
    "PROMPT = \"The sum of 2 and 2 is \"\n",
    "TEMPERATURES = [0.0, 0.5, 1.0]\n",
    "NUM_SAMPLES_PER_TEMP = 5\n",
    "MAX_NEW_TOKENS = 10\n",
    "\n",
    "if model is None or tokenizer is None:\n",
    "    raise RuntimeError(\"Model or tokenizer not loaded. Please check the previous cells.\")\n",
    "require_gpu(\"temperature\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for temp in TEMPERATURES:\n",
    "    for sample_id in range(1, NUM_SAMPLES_PER_TEMP + 1):\n",
    "        tokenized_prompt = tokenizer.encode(PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(tokenized_prompt, max_new_tokens=MAX_NEW_TOKENS, temperature=temp)\n",
    "        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        records.append({\n",
    "            \"temperature\": temp,\n",
    "            \"sample_id\": sample_id,\n",
    "            \"output\": decoded_output\n",
    "        })\n",
    "\n",
    "df_temperature = pd.DataFrame(records)\n",
    "summary_temperature = df_temperature.groupby(\"temperature\").agg({\"output\": \"nunique\"})\n",
    "\n",
    "df_temperature.to_csv(f\"{RESULTS_DIR}/task1_step2__full.csv\", index=False)\n",
    "summary_temperature.to_csv(f\"{RESULTS_DIR}/task1_step2__summary.csv\", index=False)\n",
    "print(\"Temperature sweep complete. Summary statistics:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207ab91",
   "metadata": {
    "id": "8207ab91"
   },
   "outputs": [],
   "source": [
    "### Cell 6: Experiment 2 - Effect of Input Length on Prefilling Latency\n",
    "print(\"--- Experiment 2: Input length vs. prefill latency ---\")\n",
    "\n",
    "LATENCY_INPUT_LENGTHS = [2 ** i for i in range(4, 8)]\n",
    "NUM_WARMUP = 10\n",
    "NUM_TRIALS = 10\n",
    "MAX_NEW_TOKENS = 10\n",
    "\n",
    "latency_records = []\n",
    "\n",
    "for input_length in LATENCY_INPUT_LENGTHS:\n",
    "    inputs = tokenizer(\"a\" * input_length, return_tensors=\"pt\").to(DEVICE)\n",
    "    for _ in range(NUM_WARMUP):\n",
    "        with torch.no_grad():\n",
    "            model.generate(inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "    for _ in range(NUM_TRIALS):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            model.generate(inputs, max_new_tokens=MAX_NEW_TOKENS)\n",
    "        latency_records.append({\n",
    "            \"length\": input_length,\n",
    "            \"latency\": time.time() - start_time\n",
    "        })\n",
    "\n",
    "df_latency = pd.DataFrame(latency_records)\n",
    "\n",
    "summary_latency = df_latency.groupby(\"length\").agg({\"latency\": [\"mean\", \"std\"]})\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_latency[\"length\"], df_latency[\"latency\"], marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"Input Length (tokens)\")\n",
    "plt.ylabel(\"Latency (seconds)\")\n",
    "plt.title(\"Latency vs. Input Length\")\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{FIGURES_DIR}/task1_step3__latency.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ad555",
   "metadata": {
    "id": "f64ad555"
   },
   "outputs": [],
   "source": [
    "### Cell 8: Experiment 3 - Real-time GPU Memory Usage During Generation\n",
    "print(\"--- Experiment 3: Real-time memory trace ---\")\n",
    "\n",
    "df_memory_steps = pd.DataFrame()\n",
    "\n",
    "if DEVICE.type != \"cuda\":\n",
    "    print(\"Skipped: requires GPU trace from the previous cell.\")\n",
    "else:\n",
    "    MAX_GENERATION_LENGTH = 10\n",
    "    PROMPT = \"The sum of 2 and 2 is \"\n",
    "    INPUT_LENGTH = tokenizer.encode(PROMPT, return_tensors=\"pt\").shape[1]\n",
    "\n",
    "    # TODO: prepare input_ids at the specified length\n",
    "    input_ids = tokenizer.encode(PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    class MemoryUsageCallback(StoppingCriteria):\n",
    "        def __init__(self, device):\n",
    "            self.device = device\n",
    "            self.memory_usage = []\n",
    "\n",
    "        def __call__(self, input_ids, scores, **kwargs):\n",
    "            self.memory_usage.append(torch.cuda.memory_allocated(self.device) / 1024**2)\n",
    "            return False\n",
    "\n",
    "        def increases_mb(self):\n",
    "            return self.memory_usage\n",
    "\n",
    "    callback = MemoryUsageCallback(DEVICE)\n",
    "\n",
    "    # TODO: run warmup generation without callback\n",
    "    for _ in range(NUM_WARMUP):\n",
    "        with torch.no_grad():\n",
    "            model.generate(input_ids, max_new_tokens=MAX_NEW_TOKENS)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.generate(input_ids, max_new_tokens=MAX_NEW_TOKENS, stopping_criteria=callback)\n",
    "\n",
    "    # TODO: convert callback outputs into df_memory_steps and save CSV\n",
    "    df_memory_steps = pd.DataFrame(callback.increases_mb())\n",
    "    df_memory_steps.to_csv(f\"{RESULTS_DIR}/task1_step5__memory.csv\", index=False)\n",
    "\n",
    "    # TODO: plot per-token memory increase and save the figure\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.lineplot(df_memory_steps)\n",
    "    ax.set_xlabel(\"Generation Step\")\n",
    "    ax.set_ylabel(\"Memory Usage (MB)\")\n",
    "    ax.set_title(\"Memory Usage During Generation\")\n",
    "    ax.grid(True)\n",
    "    plt.savefig(f\"{FIGURES_DIR}/task1_step5__memory.png\")\n",
    "    plt.show()\n",
    "    print(df_memory_steps.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4800e",
   "metadata": {
    "id": "d5b4800e"
   },
   "outputs": [],
   "source": [
    "### Cell 9: Space Complexity Verification\n",
    "print(\"--- Space complexity verification ---\")\n",
    "\n",
    "if DEVICE.type != \"cuda\":\n",
    "    print(\"Skipped: requires GPU trace from the previous cell.\")\n",
    "elif df_memory_steps.empty:\n",
    "    print(\"No memory trace data available. Run Experiment 3 before this analysis.\")\n",
    "else:\n",
    "    # TODO: run linear regression on df_memory_steps and report slope/intercept/R^2\n",
    "    # slope, intercept, r_value, p_value, std_err = linregress(...)\n",
    "    # print(...)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba2bfb",
   "metadata": {
    "id": "00ba2bfb"
   },
   "outputs": [],
   "source": [
    "### Cell 10: List all generated artifacts for Task 1\n",
    "print(\"Task 1 complete. Generated artifacts:\")\n",
    "\n",
    "# TODO: iterate over output directories and list generated files\n",
    "# if os.path.isdir(FIGURES_DIR):\n",
    "#     ...\n",
    "# if os.path.isdir(RESULTS_DIR):\n",
    "#     ...\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
