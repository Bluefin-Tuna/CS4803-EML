{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc693d9e",
      "metadata": {
        "id": "fc693d9e"
      },
      "outputs": [],
      "source": [
        "# CS 8803 Efficient Machine Learning\n",
        "# [First Name Last Name]\n",
        "# GT ID: [GT ID here]\n",
        "# Date: [Date here]\n",
        "#\n",
        "# Lab 02: Quantization\n",
        "#\n",
        "# This lab will guide you through the fundamental concepts of model quantization,\n",
        "# including uniform, non-uniform, and quantization-aware training (QAT).\n",
        "# You will implement these techniques from scratch and apply them to a\n",
        "# pretrained OPT-125m language model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0015a7da",
      "metadata": {
        "id": "0015a7da"
      },
      "source": [
        "# Lab Overview and Instructions\n",
        "\n",
        "In this lab, we will explore quantization with the OPT-125m model and WikiText-2 dataset to understand the basics of quantization and its implications on model performance and efficiency.\n",
        "\n",
        "### Grading\n",
        "The lab is divided into six main tasks with the following point distribution:\n",
        "-   **Task 1: Setup and Baseline (15%)**\n",
        "-   **Task 2: Uniform Quantization Implementation (20%)**\n",
        "-   **Task 3: Non-Uniform (Power-of-Two) Quantization (10%)**\n",
        "-   **Task 4: Applying and Analyzing Quantization (20%)**\n",
        "-   **Task 5: Quantization-Aware Training (QAT) (20%)**\n",
        "-   **Task 6: Advanced Quantization (15%)**\n",
        "\n",
        "### Deliverables\n",
        "You are required to submit this completed Jupyter Notebook. Ensure that you have:\n",
        "1.  Filled in all the `#TODO` sections with functional code.\n",
        "2.  Generated all the required plots and dataframes.\n",
        "3.  Written your answers and analysis in the designated markdown cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_task1_header",
      "metadata": {
        "id": "new_task1_header"
      },
      "source": [
        "# Task 1: Setup and Baseline [15%]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "295f7431",
      "metadata": {
        "id": "295f7431"
      },
      "source": [
        "### Part 1a: Download the Dataset and Model [2.5%]\n",
        "\n",
        "**Task:** Your first step is to set up the environment by loading the necessary model and dataset. You will use the Hugging Face `transformers` library to download the `facebook/opt-125m` model and the `datasets` library to load the `wikitext` dataset.\n",
        "\n",
        "-   **Model:** `facebook/opt-125m` is a small, decoder-only transformer model, suitable for experimentation without requiring heavy GPU usage.\n",
        "-   **Dataset:** `wikitext-2-raw-v1` is a standard language modeling benchmark dataset. We will use the 'train' and 'test' splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12c2bd3d",
      "metadata": {
        "id": "12c2bd3d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "from dataclasses import dataclass\n",
        "from torch.autograd import Function\n",
        "\n",
        "# --- 1. Model and Tokenizer Setup ---\n",
        "model_name = \"facebook/opt-125m\"\n",
        "\n",
        "# TODO: Load the tokenizer for the specified model.\n",
        "\n",
        "tokenizer = # TODO\n",
        "\n",
        "# OPT models don't have a default padding token, so we set it to the EOS (End Of Sentence) token.\n",
        "# This is important for batching inputs of different lengths.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# TODO: Load the pre-trained causal language model.\n",
        "# Ensure the model is moved to the correct device\n",
        "model = # TODO\n",
        "\n",
        "# --- 2. Load the dataset ---\n",
        "# We'll use the 'wikitext-2-raw-v1' version of the wikitext dataset.\n",
        "\n",
        "# TODO: Load the 'test' split for final evaluation.\n",
        "test_dataset = # TODO\n",
        "\n",
        "# TODO: Load the 'train' split, which we will use for QAT.\n",
        "train_dataset = # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f6acae5",
      "metadata": {
        "id": "5f6acae5"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "# This cell contains helper functions to tokenize and prepare the data for evaluation.\n",
        "\n",
        "def tokenize_and_chunk(texts, tokenizer, seq_length=512):\n",
        "    \"\"\"Tokenizes text and splits it into fixed-length chunks.\"\"\"\n",
        "    all_tokens = tokenizer(\n",
        "        \"\\n\\n\".join(texts),\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=False,\n",
        "    ).input_ids.squeeze()\n",
        "    data_chunks = []\n",
        "    for i in range(0, all_tokens.size(0) - seq_length, seq_length):\n",
        "        data_chunks.append(all_tokens[i : i + seq_length])\n",
        "    return data_chunks\n",
        "\n",
        "class PerplexityDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"A torch Dataset for perplexity evaluation.\"\"\"\n",
        "    def __init__(self, chunks):\n",
        "        self.chunks = chunks\n",
        "    def __len__(self):\n",
        "        return len(self.chunks)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.chunks[idx]\n",
        "\n",
        "# Create dataloaders for evaluation and training\n",
        "eval_data_chunks = tokenize_and_chunk(test_dataset[\"text\"], tokenizer, seq_length=512)\n",
        "eval_dataset = PerplexityDataset(eval_data_chunks)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=8)\n",
        "\n",
        "train_subset_size = 512\n",
        "train_data_chunks = tokenize_and_chunk(train_dataset[\"text\"][: (train_subset_size * 2)], tokenizer, seq_length=512)\n",
        "train_data_chunks = train_data_chunks[:train_subset_size]\n",
        "train_p_dataset = PerplexityDataset(train_data_chunks)\n",
        "train_dataloader = DataLoader(train_p_dataset, batch_size=8, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b321667",
      "metadata": {
        "id": "4b321667"
      },
      "source": [
        "### Part 1b: Establish Full-Precision Baseline [2.5%]\n",
        "\n",
        "**Task:** Evaluate the original, full-precision (FP32) model to establish a baseline. We will use **perplexity** as our primary metric. Answer the following question for credit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9493d51",
      "metadata": {
        "id": "b9493d51"
      },
      "source": [
        "**Question:** What is perplexity?\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48392d0b",
      "metadata": {
        "id": "48392d0b"
      },
      "outputs": [],
      "source": [
        "# Run this cell to evaluate the full precision model\n",
        "# DO NOT MODIFY THIS CELL\n",
        "# This function for evaluating perplexity is provided for you.\n",
        "\n",
        "def evaluate_perplexity(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_nll = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Calculating Perplexity\"):\n",
        "            input_ids = batch.to(device)\n",
        "            num_tokens_in_batch = input_ids.numel()\n",
        "\n",
        "            outputs = model(input_ids, labels=input_ids)\n",
        "\n",
        "            total_nll += outputs.loss.item() * num_tokens_in_batch\n",
        "            total_tokens += num_tokens_in_batch\n",
        "\n",
        "    avg_nll = total_nll / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(avg_nll))\n",
        "\n",
        "    return avg_nll, perplexity.item()\n",
        "\n",
        "\n",
        "# --- Calculate baseline perplexity ---\n",
        "loss_fp, ppl_fp = evaluate_perplexity(model, eval_dataloader, device)\n",
        "print(f\"Full precision model: loss {loss_fp:.3f}, perplexity {ppl_fp:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd0af4d",
      "metadata": {
        "id": "bdd0af4d"
      },
      "source": [
        "### Part 1c: Analyze Model Size and Weight Distribution [10%]\n",
        "\n",
        "**Task:** Before quantizing, it's important to understand the model's structure and size. You will implement helper functions to count model parameters and estimate the memory footprint after quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8878b1b",
      "metadata": {
        "id": "c8878b1b"
      },
      "outputs": [],
      "source": [
        "# Print the model architecture to understand its layers.\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e497365",
      "metadata": {
        "id": "8e497365"
      },
      "outputs": [],
      "source": [
        "# --- Helper functions to analyze model size ---\n",
        "\n",
        "def count_params(module: nn.Module):\n",
        "    \"\"\"\n",
        "    Counts the total number of parameters in a given PyTorch module.\n",
        "\n",
        "    Input:\n",
        "        module (nn.Module): The module to analyze.\n",
        "    Output:\n",
        "        int: The total number of parameters.\n",
        "    \"\"\"\n",
        "    # TODO: Calculate the total number of parameters.\n",
        "    # Hint: Iterate through module.parameters() and sum the .numel() of each parameter tensor.\n",
        "    return # TODO\n",
        "\n",
        "def estimated_weight_size_bytes(m: nn.Module, w_bits=8):\n",
        "    \"\"\"\n",
        "    Estimates the memory size of a model's weights after quantization.\n",
        "    This function assumes biases are kept in 32-bit floating point.\n",
        "\n",
        "    Inputs:\n",
        "        m (nn.Module): The model.\n",
        "        w_bits (int): The number of bits for quantized weights.\n",
        "    Output:\n",
        "        int: The estimated total size of the weights in bytes.\n",
        "    \"\"\"\n",
        "    total_bytes = 0\n",
        "    # TODO: Calculate the estimated size in bytes\n",
        "    # Iterate through all modules in the model using m.modules().\n",
        "    # For each nn.Linear layer, calculate the quantized weight size and add the bias size.\n",
        "    # Hint: bias is kept in fp32\n",
        "    return # TODO\n",
        "\n",
        "\n",
        "# Get the first OPT decoder layer parameters\n",
        "first_decoder_layer = model.model.decoder.layers[0]\n",
        "# TODO: Check the first decoder layer structure, and for each Linear layer:\n",
        "# (1) Print the linear layer name\n",
        "# (2) Print the total number of parameters\n",
        "# (3) Print the estimated weight size in bytes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_granularity_md",
      "metadata": {
        "id": "new_granularity_md"
      },
      "source": [
        "#### Visualize Weight Ranges and Granularity\n",
        "\n",
        "**Task:** Visualize the weight ranges (minimum and maximum values) for the `fc1` layer in the first decoder block of the model. This analysis helps understand why finer-grained quantization can reduce error. You will compare three different **quantization granularities**:\n",
        "\n",
        "1.  **Per-Tensor:** One min/max value for the entire weight matrix.\n",
        "2.  **Per-Channel:** One min/max value for each output channel (i.e., each row).\n",
        "3.  **Per-Group:** One min/max value for small groups of weights within each channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69644c7f",
      "metadata": {
        "id": "69644c7f"
      },
      "outputs": [],
      "source": [
        "# --- Visualize the weight ranges (min/max) for the fc1 Linear layer ---\n",
        "\n",
        "# 1. Get the weight tensor from the first fc1 layer\n",
        "fc1_layer = model.model.decoder.layers[0].fc1\n",
        "W = fc1_layer.weight.detach().cpu()\n",
        "\n",
        "# TODO: Calculate per-tensor min/max\n",
        "per_tensor_min = # TODO\n",
        "per_tensor_max = # TODO\n",
        "\n",
        "# TODO: Calculate per-channel min/max\n",
        "# For a linear layer weight (out_features, in_features), the channel axis is 0.\n",
        "per_channel_min = # TODO\n",
        "per_channel_max = # TODO\n",
        "\n",
        "# TODO: Calculate per-group min/max\n",
        "group_size = 32\n",
        "W_grouped = # TODO\n",
        "per_group_min = # TODO\n",
        "per_group_max = # TODO\n",
        "\n",
        "# --- Plotting --- (This code is provided for you)\n",
        "fig, axs = plt.subplots(2, 1, figsize=(12, 8), sharex=False)\n",
        "fig.suptitle(\"Weight Range Visualization for Different Granularities (fc1 layer)\", fontsize=16)\n",
        "\n",
        "axs[0].plot(per_channel_min.numpy(), label='Per-Channel Min', color='blue', alpha=0.7)\n",
        "axs[0].plot(per_channel_max.numpy(), label='Per-Channel Max', color='red', alpha=0.7)\n",
        "axs[0].axhline(per_tensor_min, ls='--', color='blue', label='Per-Tensor Min')\n",
        "axs[0].axhline(per_tensor_max, ls='--', color='red', label='Per-Tensor Max')\n",
        "axs[0].set_title(\"Per-Channel vs. Per-Tensor Granularity\")\n",
        "axs[0].set_xlabel(\"Output Channel Index\")\n",
        "axs[0].set_ylabel(\"Weight Value\")\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].plot(per_group_min.numpy(), label='Per-Group Min', color='green')\n",
        "axs[1].plot(per_group_max.numpy(), label='Per-Group Max', color='orange')\n",
        "axs[1].set_title(f\"Per-Group Granularity (Group Size = {group_size})\")\n",
        "axs[1].set_xlabel(\"Group Index\")\n",
        "axs[1].set_ylabel(\"Weight Value\")\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_granularity_analysis",
      "metadata": {
        "id": "new_granularity_analysis"
      },
      "source": [
        "**Question:** Based on the visualization, explain how finer granularity (like per-channel or per-group) can reduce quantization error compared to per-tensor quantization.\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_task2_header",
      "metadata": {
        "id": "new_task2_header"
      },
      "source": [
        "# Task 2: Uniform Quantization Implementation [20%]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df9d8a8",
      "metadata": {
        "id": "3df9d8a8"
      },
      "source": [
        "### Part 2a: Implement Uniform Quantization Logic [20%]\n",
        "\n",
        "**Task:** You will now implement the core logic for uniform quantization. This involves calculating scale factors and zero-points, and then using them to map high-precision floating-point numbers to lower-precision integers.\n",
        "\n",
        "You will use the **Straight-Through Estimator (STE)** to allow gradients to pass through the non-differentiable `round` function, which is crucial for quantization-aware training later.\n",
        "\n",
        "#### Hint:\n",
        "-   **Affine (Asymmetric):** Maps values to $[q_{\\min}, q_{\\max}]$.\n",
        "    \n",
        "-   **Symmetric:** Maps values to $[-(2^{b-1}), 2^{b-1}-1]$.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b9ac06",
      "metadata": {
        "id": "35b9ac06"
      },
      "outputs": [],
      "source": [
        "# STE function (provided)\n",
        "class _STERound(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        return torch.round(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output\n",
        "\n",
        "ste_round = _STERound.apply\n",
        "\n",
        "# Quantization configuration (provided)\n",
        "@dataclass\n",
        "class QuantConfig:\n",
        "    num_bits: int = 8\n",
        "    symmetric: bool = True\n",
        "    per_channel: bool = False\n",
        "    channel_axis: int = 0\n",
        "    group_size: int = None\n",
        "\n",
        "# Helper for getting min/max (provided)\n",
        "def get_min_max(x, axis=None):\n",
        "    if axis is None:\n",
        "        return torch.min(x), torch.max(x)\n",
        "    else:\n",
        "        return torch.amin(x, dim=axis, keepdim=True), torch.amax(x, dim=axis, keepdim=True)\n",
        "\n",
        "def get_scale_zero_point(x_min, x_max, cfg, qmin, qmax, device='cpu'):\n",
        "    \"\"\"\n",
        "    Calculates the scale and zero-point for quantization.\n",
        "    \"\"\"\n",
        "    if cfg.symmetric:\n",
        "        # TODO: Implement symmetric quantization scale and zero_point\n",
        "        scale = # TODO\n",
        "        zero_point = # TODO\n",
        "    else:\n",
        "        # TODO: Implement affine (asymmetric) quantization scale and zero_point.\n",
        "        scale = # TODO\n",
        "        zero_point = # TODO\n",
        "\n",
        "    # Handle the case where scale is zero to avoid division by zero errors.\n",
        "    scale = torch.where(scale == 0, torch.tensor(1.0, device=device), scale)\n",
        "    return scale.to(device), zero_point.to(device)\n",
        "\n",
        "def quantize_tensor(x, scale, zero_point, qmin, qmax):\n",
        "    \"\"\"\n",
        "    Quantizes tensor x and then dequantizes it (simulating the quantization error).\n",
        "    \"\"\"\n",
        "    # TODO: Implement the quantization and dequantization formulas.\n",
        "    # Hint: Use the ste_round function for the rounding operation.\n",
        "    x_q = # TODO\n",
        "    x_q_clamped = # TODO: Hint: clamp the quantized values to be within [qmin, qmax].\n",
        "    x_dq = # TODO\n",
        "    return x_dq\n",
        "\n",
        "def uniform_quantize_dequantize(x: torch.Tensor, cfg: QuantConfig):\n",
        "    \"\"\"\n",
        "    A wrapper function for uniform quantization-dequantization.\n",
        "    \"\"\"\n",
        "    if cfg.symmetric:\n",
        "        # TODO: Define qmin and qmax for symmetric quantization.\n",
        "        qmin, qmax = # TODO\n",
        "    else:\n",
        "        # TODO: Define qmin and qmax for affine (asymmetric) quantization.\n",
        "        qmin, qmax = # TODO\n",
        "\n",
        "    # Hint: Use get_min_max, get_scale_zero_point, quantize_tensor functions\n",
        "    if cfg.group_size is not None and x.ndim > 1:\n",
        "        # TODO: Implement the logic for per-group quantization.\n",
        "\n",
        "        return x_dq # dequantized tensor\n",
        "\n",
        "    elif cfg.per_channel and x.ndim > 1:\n",
        "        # TODO: Implement per-channel quantization.\n",
        "\n",
        "        return x_dq # dequantized tensor\n",
        "    else:\n",
        "        # TODO: Implement per-tensor quantization.\n",
        "\n",
        "        return x_dq # dequantized tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_task3_header",
      "metadata": {
        "id": "new_task3_header"
      },
      "source": [
        "# Task 3: Non-Uniform (Power-of-Two) Quantization [10%]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536f84a5",
      "metadata": {
        "id": "536f84a5"
      },
      "source": [
        "### Part 3a: Implement Power-of-Two Quantization [10%]\n",
        "\n",
        "A simple **non-uniform** scheme is **power-of-two (PoT) quantization**, where values are mapped to the nearest $\\pm 2^k$. This concentrates quantization levels near zero, which often aligns well with the distribution of weights in neural networks.\n",
        "\n",
        "**Task:** Implement the PoT quantization logic. A key challenge is handling cases where the range of exponents (`k`) exceeds the number of available quantization levels ($2^b-1$). In such cases, you must scale the exponents to fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b001e1e",
      "metadata": {
        "id": "0b001e1e"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PoTQuantConfig:\n",
        "    num_bits: int = 8\n",
        "    per_channel: bool = False\n",
        "    channel_axis: int = 0\n",
        "    eps: float = 1e-8\n",
        "\n",
        "def pot_quantize_dequantize(x: torch.Tensor, cfg: PoTQuantConfig):\n",
        "    \"\"\"\n",
        "    Power-of-Two (PoT) quantization and dequantization.\n",
        "    Maps values to the nearest signed power-of-two.\n",
        "    \"\"\"\n",
        "    def quantize_block(block):\n",
        "        # Avoid log(0) by clamping the minimum magnitude.\n",
        "        mag = block.abs().clamp(min=cfg.eps)\n",
        "\n",
        "        # TODO: Calculate exponents\n",
        "        exponents = # TODO\n",
        "\n",
        "        kmax, kmin = exponents.max(), exponents.min()\n",
        "        max_levels = (1 << cfg.num_bits) - 1\n",
        "        span = (kmax - kmin + 1).clamp(min=1)\n",
        "\n",
        "        # TODO: If the required exponent span is greater than the available levels,\n",
        "        # you must scale the exponents to fit.\n",
        "        if span > max_levels:\n",
        "            # 1. Calculate the scaling factor.\n",
        "            scale = # TODO\n",
        "            # 2. Scale the exponents: e_scaled = round((e - kmin) * scale) + kmin\n",
        "            exponents = # TODO\n",
        "\n",
        "        # TODO: Reconstruct the dequantized value\n",
        "        dq = # TODO\n",
        "\n",
        "        # Ensure that original zeros remain zero.\n",
        "        return torch.where(block == 0, torch.zeros_like(dq), dq)\n",
        "\n",
        "    if cfg.per_channel and x.ndim > 1:\n",
        "        # TODO: Implement per-channel PoT quantization.\n",
        "        return x_dq # dequantized tensor\n",
        "    else:\n",
        "        return quantize_block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_task4_header",
      "metadata": {
        "id": "new_task4_header"
      },
      "source": [
        "# Task 4: Applying and Analyzing Quantization [20%]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05d401f",
      "metadata": {
        "id": "d05d401f"
      },
      "source": [
        "### Part 4a: Create a Fake Quantized Linear Layer [5%]\n",
        "\n",
        "**Task:** To apply our quantization functions to the model, we need to wrap the existing `nn.Linear` layers. You will implement `QuantLinearFake`, a module that replaces a linear layer and applies \"fake\" (or \"simulated\") quantization to its weights and activations during the forward pass. This means the computations are still done in FP32, but the values are rounded to simulate the precision loss of lower-bit representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eebb875",
      "metadata": {
        "id": "2eebb875"
      },
      "outputs": [],
      "source": [
        "class QuantLinearFake(nn.Module):\n",
        "    def __init__(self, linear: nn.Linear,\n",
        "                 w_method=\"uniform\", w_cfg: dict = None,\n",
        "                 a_method=\"uniform\", a_cfg: dict = None):\n",
        "        super().__init__()\n",
        "        self.inner = linear\n",
        "        self.w_method, self.a_method = w_method, a_method\n",
        "        self.w_cfg, self.a_cfg = w_cfg or {}, a_cfg or {}\n",
        "        self.weight, self.bias = self.inner.weight, self.inner.bias\n",
        "\n",
        "    def quantize_weights(self, W):\n",
        "        # TODO: Implement weight quantization logic.\n",
        "        # Based on self.w_method, call the appropriate quantization function\n",
        "        if self.w_method == \"uniform\":\n",
        "            return # TODO\n",
        "        elif self.w_method == \"pot\":\n",
        "            return # TODO\n",
        "        else:\n",
        "            return W\n",
        "\n",
        "    def quantize_acts(self, x):\n",
        "        if self.a_method == \"uniform\" and \"num_bits\" in self.a_cfg:\n",
        "            cfg = QuantConfig(**self.a_cfg)\n",
        "            return uniform_quantize_dequantize(x, cfg)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This forward pass is provided for you.\n",
        "        xq = self.quantize_acts(x)\n",
        "        Wq = self.quantize_weights(self.weight)\n",
        "        return F.linear(xq, Wq, self.bias)\n",
        "\n",
        "# This wrapper function to replace linear modules is provided for you.\n",
        "def wrap_linear_modules(module: nn.Module, should_wrap_fn):\n",
        "    for name, child in list(module.named_children()):\n",
        "        if isinstance(child, nn.Linear):\n",
        "            cfg = should_wrap_fn(name, child)\n",
        "            if cfg is not None:\n",
        "                wrapped = QuantLinearFake(child, **cfg)\n",
        "                setattr(module, name, wrapped)\n",
        "        else:\n",
        "            wrap_linear_modules(child, lambda n,m: should_wrap_fn(f\"{name}.{n}\", m))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d96266",
      "metadata": {
        "id": "e9d96266"
      },
      "source": [
        "### Part 4b: Quantization Experiments and Analysis [15%]\n",
        "\n",
        "**Task:** Now you will use your implemented functions to run a series of experiments (a \"sweep\") to analyze the impact of different quantization settings. You will compare:\n",
        "\n",
        "1.  **Quantization Methods:** Uniform Symmetric vs. Uniform Asymmetric vs. Power-of-Two (PoT).\n",
        "2.  **Bit Precision:** 16, 8, 4, and 2-bit weight quantization.\n",
        "3.  **Quantization Granularity:** Per-tensor vs. per-channel vs. per-group.\n",
        "\n",
        "The default setting is : 8-bit weight quantization, full precision activation, per-group quantization, Uniform Asymmetric\n",
        "\n",
        "Fill in the `sweep` list below to define your experiments. Then, run the cell and analyze the resulting dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "443f7cdc",
      "metadata": {
        "id": "443f7cdc"
      },
      "outputs": [],
      "source": [
        "# Helper functions for running the sweep (provided)\n",
        "def is_opt_linear_of_interest(full_name: str):\n",
        "    return any(p in [\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"fc1\",\"fc2\"] for p in full_name.split(\".\"))\n",
        "\n",
        "def make_quantized_copy(base_model, method, w_bits, a_bits=None, **kwargs):\n",
        "    m = deepcopy(base_model).cpu(); m.eval()\n",
        "    w_cfg = dict(num_bits=w_bits, **kwargs)\n",
        "    a_cfg = dict(num_bits=a_bits, symmetric=False) if a_bits else {}\n",
        "    def should_wrap(name, module):\n",
        "        if is_opt_linear_of_interest(name):\n",
        "            return {\"w_method\": method, \"w_cfg\": w_cfg, \"a_method\": \"uniform\", \"a_cfg\": a_cfg}\n",
        "    wrap_linear_modules(m, should_wrap)\n",
        "    return m\n",
        "\n",
        "def eval_on_device(m, loader, device):\n",
        "    m.to(device)\n",
        "    loss, ppl = evaluate_perplexity(m, loader, device)\n",
        "    m.to(\"cpu\"); gc.collect()\n",
        "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
        "    return loss, ppl\n",
        "\n",
        "# TODO: Define your experiments in the sweep list.\n",
        "# Each entry is a dictionary specifying the parameters for make_quantized_copy.\n",
        "sweep = [\n",
        "    # Example: {\"method\": \"uniform\", \"w_bits\": 8, \"symmetric\": False, \"granularity\": \"per_group\", \"group_size\": 32},\n",
        "\n",
        "    # Task 1: Compare Quantization Methods (uniform symmetric, uniform asymmetric, power-of-two)\n",
        "\n",
        "    # Task 2: Compare Precision (16,8,4,2 bit weight quantization)\n",
        "\n",
        "    # Task 3: Compare Granularity (per-tensor, per-channel, per-group)\n",
        "\n",
        "]\n",
        "\n",
        "results = []\n",
        "for params in sweep:\n",
        "    gran = params.get(\"granularity\", \"per_tensor\")\n",
        "    params[\"per_channel\"] = (gran == \"per_channel\")\n",
        "    if \"granularity\" in params: del params[\"granularity\"]\n",
        "\n",
        "    qm = make_quantized_copy(model, **params)\n",
        "    est_size = estimated_weight_size_bytes(qm, w_bits=params['w_bits'])\n",
        "    loss, ppl = eval_on_device(qm, eval_dataloader, device)\n",
        "\n",
        "    res_key = {**params, \"granularity\": gran, \"loss\": loss, \"ppl\": ppl, \"size_mb\": est_size / 1e6}\n",
        "    results.append(res_key)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(\"--- Experiment Results ---\")\n",
        "print(f\"Baseline FP32 Perplexity: {ppl_fp:.3f}\")\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b99e5f2",
      "metadata": {
        "id": "5b99e5f2"
      },
      "source": [
        "### Analysis of Results\n",
        "\n",
        "Based on the dataframe above, answer the following questions.\n",
        "\n",
        "**1. Granularity Trade-offs:** What is the relationship between quantization granularity (per-tensor, per-channel, per-group) and model perplexity? Does a finer granularity always result in better performance? What might be the hardware/efficiency trade-offs?\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**\n",
        "\n",
        "**2. Quantization Methods:** Compare Uniform Symmetric, Uniform Asymmetric, and Non-uniform (PoT) quantization at 8 bits. Which performed best? Why might one be preferred over another in terms of hardware friendliness?\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**\n",
        "\n",
        "**3. Precision vs. Performance:** Describe the trend you observe as you decrease the number of weight bits (from 16 down to 2). At what point does the model's performance degrade significantly?\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**\n",
        "\n",
        "**4. Layer Sensitivity (Conceptual):** We don't quantize the bias, LayerNorm, embedding, or lm_head layers. Why is this a common practice? (Hint: think about parameter count, sensitivity to precision, and expected compute/memory savings).\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "new_task5_header",
      "metadata": {
        "id": "new_task5_header"
      },
      "source": [
        "# Task 5: Quantization-Aware Training (QAT) [20%]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2792f643",
      "metadata": {
        "id": "2792f643"
      },
      "source": [
        "### Part 5a: Implement a QAT Training Loop [10%]\n",
        "\n",
        "Post-Training Quantization (PTQ), which you performed in the previous task, is simple but can lead to significant accuracy degradation, especially at very low bit-widths. **Quantization-Aware Training (QAT)** is a technique to mitigate this by simulating the quantization effects during a short fine-tuning phase. This allows the model to adapt its weights to the quantization process.\n",
        "\n",
        "**Task:** Implement a simple QAT training loop. Because we used the Straight-Through Estimator (STE) in our quantization functions, gradients can flow through the fake-quantized layers, enabling us to fine-tune the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f669336",
      "metadata": {
        "id": "2f669336"
      },
      "outputs": [],
      "source": [
        "def qat_training(model, dataloader, device, num_epochs=3, learning_rate=1e-5):\n",
        "    \"\"\"\n",
        "    Simple QAT training loop.\n",
        "    \"\"\"\n",
        "    model.train() # Set the model to training mode\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"QAT Epoch {epoch+1}/{num_epochs}\"):\n",
        "            input_ids = batch.to(device)\n",
        "\n",
        "            # TODO: Implement the training step\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = # TODO (Hint: The model computes loss when `labels` are provided).\n",
        "            loss = outputs.loss\n",
        "            # TODO\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"QAT Epoch {epoch+1} - Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c04c45",
      "metadata": {
        "id": "93c04c45"
      },
      "source": [
        "### Part 5b: Run QAT and Compare with PTQ [10%]\n",
        "\n",
        "**Task:** We revisit the bit precision experiment from the previous task: (16, 8, 4, 2) bit weight quantization, full precision activation, uniform asymmetric, per-group quantization. Apply your QAT training loop for 3 epochs and compare the final perplexity against the PTQ (Post-Training Quantization) result for the same setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22a3ae4e",
      "metadata": {
        "id": "22a3ae4e"
      },
      "outputs": [],
      "source": [
        "# --- Define the quantization settings for the QAT experiment ---\n",
        "# Let's try to improve a challenging 4-bit model.\n",
        "QAT_PARAMS = {\n",
        "    \"method\": \"uniform\",\n",
        "    \"w_bits\": 4,\n",
        "    \"symmetric\": False,\n",
        "    \"granularity\": \"per_group\",\n",
        "    \"group_size\": 32\n",
        "}\n",
        "gran = QAT_PARAMS.pop(\"granularity\") # Handle granularity key for our function\n",
        "QAT_PARAMS[\"per_channel\"] = (gran == \"per_channel\")\n",
        "\n",
        "# --- 1. Create and evaluate the PTQ model (the baseline) ---\n",
        "print(\"--- Creating baseline PTQ model for comparison ---\")\n",
        "# TODO: Create the PTQ model using make_quantized_copy and QAT_PARAMS.\n",
        "ptq_model = # TODO\n",
        "loss_ptq, ppl_ptq = eval_on_device(ptq_model, eval_dataloader, device)\n",
        "\n",
        "# --- 2. Create and train the QAT model ---\n",
        "print(f\"\\n--- Starting QAT for W{QAT_PARAMS['w_bits']} {gran} model ---\")\n",
        "# TODO: Create the initial model for QAT using the same parameters.\n",
        "qat_model = # TODO\n",
        "qat_model.to(device) # Move to device for training\n",
        "\n",
        "print(\"Starting QAT fine-tuning...\")\n",
        "qat_model = qat_training(qat_model, train_dataloader, device, num_epochs=1, learning_rate=1e-5)\n",
        "\n",
        "# --- 3. Evaluate the final QAT model ---\n",
        "print(\"Evaluating final QAT model...\")\n",
        "loss_qat, ppl_qat = eval_on_device(qat_model, eval_dataloader, device)\n",
        "\n",
        "# --- 4. Print Final Comparison ---\n",
        "print(\"\\n--- QAT vs. PTQ Results ---\")\n",
        "print(f\"Baseline PTQ Perplexity: {ppl_ptq:.4f}\")\n",
        "print(f\"QAT-finetuned Perplexity: {ppl_qat:.4f}\")\n",
        "print(\"-\" * 20)\n",
        "print(f\"Perplexity Improvement with QAT: {ppl_ptq - ppl_qat:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e4546f",
      "metadata": {
        "id": "97e4546f"
      },
      "source": [
        "### Analysis of QAT Results\n",
        "\n",
        "**Question:** Compare the perplexity of the PTQ and QAT results across different bit-precisions (16, 8, 4, 2) bit weight quantization. Did QAT successfully recover some of the performance lost during quantization? Briefly explain why fine-tuning with simulated quantization helps the model adapt. How do PTQ and QAT perform compared to the full precision model? Do they improve? If so, why? If not, explain the performance gap.\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56abc703",
      "metadata": {
        "id": "56abc703"
      },
      "source": [
        "# Task 6: Advanced Quantization [15%]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad1a10b",
      "metadata": {
        "id": "9ad1a10b"
      },
      "source": [
        "Get familiar with the technical details of more advanced quantization method: CPT (ICLRâ€™21)\n",
        "Each answer should be 3-4 sentences long.\n",
        "\n",
        "\n",
        "**1. Q1 [5%]:** What is the motivation and key insight of CPT?\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**\n",
        "\n",
        "**2. Q2 [5%]:** Summarize the methodology of CPT.\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**\n",
        "\n",
        "**3. Q3 [5%]:** What are the pros and cons of prior works and why is CPT better?\n",
        "\n",
        "**[WRITE YOUR ANSWER HERE]**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}